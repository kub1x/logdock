# Inspired by: https://github.com/sheepkiller/kafka-manager-docker/blob/master/docker-compose.yml
#      and by: https://github.com/deviantony/docker-elk/blob/master/docker-compose.yml
version: '3'
services:

  zookeeper:
    build: zookeeper/
    ports:
      - "2181"
    networks:
      - mq

  kafka:
    build: kafka/
    ports:
      - "9092:9092"
    networks:
      - mq
    environment:
      # NOTE specify these to prevent autogeneration
      #KAFKA_ADVERTISED_HOST_NAME: ""
      #KAFKA_BROKER_ID: ""
      #KAFKA_ADVERTISED_PORT: ""
      # NOTE local setting of topics
      KAFKA_CREATE_TOPICS: "test:1:1,os-t1x-in:1:1"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_DELETE_TOPIC_ENABLE: 1 #true
      KAFKA_LOG_RETENTION_HOUS: 1
      KAFKA_MESSAGE_MAX_BYTES: 10000000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10000000
      KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS: 60000
      KAFKA_NUM_PARTITIONS: 2
      KAFKA_DELETE_RETENTION_MS: 1000
      # NOTE the "swarm" config
      #KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      #KAFKA_ADVERTISED_PROTOCOL_NAME: OUTSIDE
      #KAFKA_ADVERTISED_PORT: 9094
      #KAFKA_PROTOCOL_NAME: INSIDE
      #KAFKA_PORT: 9092
      # NOTE autodetect hostname
      #HOSTNAME_COMMAND: "docker info | grep ^Name: | cut -d' ' -f 2"
      # NOTE autodetect generated broker_id
      #BROKER_ID_COMMAND: "hostname | awk -F'-' '{print $2}'"
    depends_on:
      - zookeeper
    #volumes:
    # NOTE can specify kafka config files
    #  - ./kafka/config/server.properties:/opt/kafka/config/server.properties
    #  - ./kafka/config/log4j.properties:/opt/kafka/config/log4j.properties
    #
    # NOTE maybe previous could be as follows, because $KAFKA_HOME is specified
    #      in the wurstmeister/kafka image's Dockerfile using ENV command.
    #      See: https://docs.docker.com/compose/compose-file/#variable-substitution
    #
    #  - ./kafka/config/server.properties:$$KAFKA_HOME/config/server.properties
    #  - ./kafka/config/log4j.properties:$$KAFKA_HOME/config/log4j.properties
    #
    # TODO why mount docker.sock?
    #  - /var/run/docker.sock:/var/run/docker.sock

  kafka-manager:
    build: kafka-manager/
    ports:
      - "9000:9000"
    networks:
      - mq
    environment:
      ZK_HOSTS: zookeeper:2181
      # NOTE see followign link to explani APPLICATION_SECRET
      # https://github.com/yahoo/kafka-manager/blob/master/conf/application.conf
      APPLICATION_SECRET: letmein
      KM_ARGS: -Djava.net.preferIPv4Stack=true
      # NOTE we can specify path to config file
      #KM_CONFIG: /path/to/km/file.conf
    depends_on:
      - zookeeper
      - kafka
    # NOTE we can override default kafka config by remounting the conf directory
    #      hopefully the $$ sytnax works correctly on volumes =j
    #volumes:
    #  - ./kafka-manager/config:/kafka-manager-$${KM_VERSION}/conf

  elasticsearch:
    build: elasticsearch/
    volumes:
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      # NOTE you can mount a local file as a ES data storage
      #- ./elasticsearch/data-storage:/usr/share/elasticsearch/data
      #
      # NOTE alternatively you can create a volume container "elasticsearch-volume"
      # specify a local volume in its Dockerfile using:
      #   VOLUME /usr/share/elasticsearch/data
      # and connnect them using:
      #   docker ... --volumes_from elasticsearch-volume ... elasticsearch
      # and find where it stores data in "Volumes" section of this commands output:
      #   docker inspect elasticsearch-volume
      #
      # NOTE docker-compose makes this simple using "volumes:" root section of this file below:
      #- elasticsearch-volume: /usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xmx256m -Xms256m"
    networks:
      - elk

  logstash:
    build: logstash/
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    ports:
      - "5000:5000" # TCP input
      - "9600:9600" # API endpoint
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    networks:
      - elk
      - mq
    depends_on:
      - elasticsearch
      - kafka
      - zookeeper

  kibana:
    build: kibana/
    volumes:
      - ./kibana/config/:/usr/share/kibana/config
    ports:
      - "5601:5601"
    networks:
      - elk
    depends_on:
      - elasticsearch

networks:
  elk:
    driver: bridge
  mq:
    driver: bridge

# NOTE enable for having autocreated volumes separated from containers
#volumes:
#  elasticsearch-volume:
